
# ğŸ¤– LangChain Tool Calling Agent with Semantic Routing & Memory

This project is an intelligent multi-tool conversational AI system built using **LangChain**, **Groq LLM**, **FAISS Vector Database**, **FastAPI backend**, and **Streamlit frontend**.

It supports:
- âœ… Multi-turn conversations with memory
- âœ… Semantic routing using vector similarity
- âœ… Automatic tool selection via agent reasoning
- âœ… Student marks storage & retrieval
- âœ… Emotion-aware responses (positive / negative)
- âœ… Safety handling for sensitive queries
- âœ… API + Web UI integration

---
## Demo Video
https://drive.google.com/file/d/1Qrh_3ah9itbfgYXtPaEivJbiWM09syhM/view?usp=sharing

## ğŸ“ Project Folder Structure

```

LangChainToolCallingAgentCode/
â”‚
â”‚
â”œâ”€â”€ agent_backend.py           # FastAPI backend server
â”œâ”€â”€ agent_frontend.py          # Streamlit frontend UI
â”œâ”€â”€ main.py                    # Core AI agent logic (Brain of project)
â”œâ”€â”€ check.py                   # Dependency / environment testing file
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ .gitignore                 # Git ignore rules
â””â”€â”€ README.md                  # Project documentation

```

---

## ğŸ§  Overall Project Architecture

<img width="2496" height="1468" alt="autodraw 28_11_2025" src="https://github.com/user-attachments/assets/df971e5c-f4c3-4fc5-bbb6-0c36366c6ff0" />

---

## ğŸ”„ Detailed Internal Workflow

### Step 1: User Sends a Message
User types a message in **Streamlit UI** â†’ It is sent to FastAPI via `/chat` API.

![WhatsApp Image 2025-11-26 at 22 09 00_de5a47bb](https://github.com/user-attachments/assets/a13a0d79-9c5a-481b-9a9e-0bf09f7a94bd)

![WhatsApp Image 2025-11-26 at 22 08 08_e10aa8df](https://github.com/user-attachments/assets/171e741c-6fa2-4376-bb62-7c516e0916dc)

![WhatsApp Image 2025-11-26 at 22 08 42_0f462c70](https://github.com/user-attachments/assets/1520a027-63ac-41e6-8648-0793add6f924)

<img width="1893" height="809" alt="session-history" src="https://github.com/user-attachments/assets/a8ecbee5-e6e1-44a7-9e90-beea4ae9173a" />


### Step 2: Router Memory Stores the Message
```python
get_memory(session_id).save_context({"input": message}, {"output": ""})
````

This stores the user message for:

* Conversation continuity
* Follow-up question support
* Emotional context tracking
* `history` command support

âœ… **Memory is for CONTEXT, not routing.**

---

### Step 3: Vector Database Stores the Message

```python
store_in_vector_db(message, "raw_user_input")
```

This stores the message as an embedding for:

* Semantic similarity
* Intent detection
* Repeated pattern learning

âœ… **Vector DB is ONLY for semantic routing.**

---

### Step 4: Semantic Router Determines Intent

```python
intent = semantic_router(message)
```

* Converts user query into embeddings
* Compares with FAISS stored vectors
* Detects closest intent:

  * positive
  * negative
  * academic
  * safety
  * generic

---

### Step 5: Tool Selection & Execution

```python
if intent == "positive":
    reply = positive_prompt_tool(...)
elif intent == "negative":
    reply = negative_prompt_tool(...)
elif intent == "academic":
    reply = student_marks_tool(...)
elif intent == "safety":
    reply = suicide_safety_tool(...)
else:
    reply = agent.run(message)
```

* If intent matches a tool â†’ that tool is executed directly
* Otherwise â†’ LangChain **agent reasoning** selects the best tool automatically

---

### Step 6: LLM Generates Final Response

Groq LLM (`llama-3.3-70b-versatile`) generates the response using:

* Current user input
* Previous memory
* Tool output (if any)

---

### Step 7: Response Stored Again

```python
get_memory(session_id).save_context({"input": message}, {"output": reply})
store_in_vector_db(reply, intent)
```

* Final answer saved in memory
* Final answer stored in FAISS

---

### Step 8: Response Returned to Frontend

Streamlit displays:

* User message
* Agent reply
* Full chat history (if requested)

---

## ğŸ“‚ File-wise Detailed Explanation

---

### âœ… `main.py` â€” Core AI Brain (MOST IMPORTANT)

This file contains:

* Groq LLM initialization
* Router memory setup
* FAISS vector database
* Semantic router
* Tool definitions
* Agent registration
* Main chat pipeline

It controls:

* âœ… Intent detection
* âœ… Tool execution
* âœ… Memory storage
* âœ… Vector DB updates
* âœ… Agent reasoning
* âœ… Final response generation

Without this file â†’ **Project will not work.**

---

### âœ… `agent_backend.py` â€” FastAPI Backend

This file:

* Exposes `/chat` API
* Accepts user messages
* Calls `chat()` function from `main.py`
* Returns AI response as JSON

Acts as:

> **Bridge between frontend and AI brain**

---

### âœ… `agent_frontend.py` â€” Streamlit Frontend

This file:

* Creates UI for chat
* Sends requests to FastAPI
* Shows chat history
* Displays responses in real-time

Acts as:

> **User Interface of the AI system**

---

### âœ… `.env` â€” Environment Variables

Contains:

```
GROQ_API_KEY=your_api_key_here
```

Used securely to authenticate Groq LLM.

---

### âœ… `requirements.txt` â€” Dependencies

Contains:

* LangChain core libraries
* Groq SDK
* FAISS
* Sentence Transformers
* FastAPI
* Streamlit
* Torch
* dotenv

Used for:

```bash
pip install -r requirements.txt
```

---

### âœ… `check.py` â€” Environment Test File

Used to:

* Verify imports
* Verify LangChain installation
* Test memory availability

Not used in production.

---

### âœ… `venv/` â€” Virtual Environment

Isolated Python environment to:

* Avoid version conflicts
* Ensure stable execution

---

### âœ… `.gitignore`

Prevents pushing:

* `venv/`
* `.env`
* `__pycache__/`
  to GitHub.

---

## ğŸ›  Tools in the Project

| Tool Name        | Purpose                           |
| ---------------- | --------------------------------- |
| PositiveResponse | Motivation & happy replies        |
| NegativeResponse | Emotional & empathetic replies    |
| StudentMarks     | Stores & retrieves marks          |
| SafetyTool       | Handles suicide/sensitive queries |

The agent automatically decides which tool to call.

---

## ğŸ§­ Key Concept Differences

| Component     | Role                       |
| ------------- | -------------------------- |
| Router Memory | Conversation context       |
| Vector DB     | Semantic intent detection  |
| Agent         | Tool selection & reasoning |
| Tools         | Task execution             |
| LLM           | Natural language response  |

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Activate Virtual Environment

```bash
venv\Scripts\activate
```

### 2ï¸âƒ£ Install Requirements

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Backend (FastAPI)

```bash
uvicorn agent_backend:app --reload
```

Backend runs at:

```
http://127.0.0.1:8000
```

### 4ï¸âƒ£ Run Frontend (Streamlit)

```bash
streamlit run agent_frontend.py
```

---

## ğŸ§ª Example Queries

```
motivate me
i am feeling sad
store marks 93 in maths
get the marks in maths
history
```

---

## âœ… Final Summary

This project demonstrates:

* âœ… Intelligent tool-calling with LangChain
* âœ… Semantic routing with FAISS
* âœ… Multi-session memory handling
* âœ… Emotion-aware conversational AI
* âœ… Full-stack AI system (UI + API + LLM)

---

### ğŸ‘©â€ğŸ’» Author

**Chhavi**
Final Year B.Tech | AI & DevOps Enthusiast

```


